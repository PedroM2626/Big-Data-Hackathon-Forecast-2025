{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo de Previs√£o de Vendas (Forecast) - One-Click Order\n",
    "## Sistema para prever quantidade semanal de vendas por PDV/SKU\n",
    "**Objetivo:** Apoiar reposi√ß√£o de estoque para 4 semanas de janeiro/2023  \n",
    "**Base:** Hist√≥rico de vendas de 2022  \n",
    "**Contexto:** One-Click Order - Sistema de reposi√ß√£o autom√°tica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa√ß√£o das bibliotecas necess√°rias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Bibliotecas para s√©ries temporais e forecasting\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit, cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Bibliotecas para an√°lise temporal\n",
    "from datetime import datetime, timedelta\n",
    "import calendar\n",
    "\n",
    "# Bibliotecas para visualiza√ß√£o\n",
    "try:\n",
    "    import plotly.express as px\n",
    "    import plotly.graph_objects as go\n",
    "    from plotly.subplots import make_subplots\n",
    "    PLOTLY_AVAILABLE = True\n",
    "except ImportError:\n",
    "    PLOTLY_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è Plotly n√£o dispon√≠vel - usando apenas matplotlib\")\n",
    "\n",
    "# Configura√ß√µes\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "print(\"‚úì Bibliotecas importadas com sucesso!\")\n",
    "print(\"üéØ OBJETIVO: Modelo de Forecast para Reposi√ß√£o de Estoque\")\n",
    "print(\"üìÖ META: Prever vendas para 5 semanas de janeiro/2023\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Carregamento dos Dados Hist√≥ricos (2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìÇ CARREGAMENTO DOS DADOS DE VENDAS 2022\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Caminhos dos arquivos\n",
    "arquivos = [\n",
    "    \"Dados/part-00000-tid-6364321654468257203-dc13a5d6-36ae-48c6-a018-37d8cfe34cf6-263-1-c000.snappy.parquet\",\n",
    "    \"Dados/part-00000-tid-5196563791502273604-c90d3a24-52f2-4955-b4ec-fb143aae74d8-4-1-c000.snappy.parquet\",\n",
    "    \"Dados/part-00000-tid-2779033056155408584-f6316110-4c9a-4061-ae48-69b77c7c8c36-4-1-c000.snappy.parquet\"\n",
    "]\n",
    "\n",
    "# Carregamento dos dados hist√≥ricos\n",
    "dataframes = []\n",
    "for i, arquivo in enumerate(arquivos):\n",
    "    try:\n",
    "        df_temp = pd.read_parquet(arquivo)\n",
    "        df_temp['arquivo_origem'] = f'arquivo_{i+1}'\n",
    "        dataframes.append(df_temp)\n",
    "        print(f\"‚úì Arquivo {i+1}: {df_temp.shape[0]} linhas, {df_temp.shape[1]} colunas\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro ao carregar arquivo {i+1}: {e}\")\n",
    "\n",
    "# Uni√£o dos dataframes\n",
    "if dataframes:\n",
    "    df_raw = pd.concat(dataframes, ignore_index=True)\n",
    "    print(f\"\\nüìä Dataset hist√≥rico: {df_raw.shape[0]} linhas, {df_raw.shape[1]} colunas\")\n",
    "else:\n",
    "    print(\"‚ùå Nenhum arquivo foi carregado com sucesso!\")\n",
    "    \n",
    "print(\"\\nüîç Estrutura dos dados:\")\n",
    "display(df_raw.info())\n",
    "print(\"\\nüìã Primeiras linhas:\")\n",
    "display(df_raw.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. An√°lise e Interpreta√ß√£o da Estrutura dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîç AN√ÅLISE DA ESTRUTURA DOS DADOS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Identificar colunas relevantes para forecast\n",
    "print(\"üìä An√°lise das colunas dispon√≠veis:\")\n",
    "for col in df_raw.columns:\n",
    "    unique_vals = df_raw[col].nunique()\n",
    "    data_type = df_raw[col].dtype\n",
    "    null_count = df_raw[col].isnull().sum()\n",
    "    print(f\"   {col}: {unique_vals} valores √∫nicos, tipo: {data_type}, nulos: {null_count}\")\n",
    "\n",
    "# Identificar poss√≠veis colunas chave\n",
    "possible_date_cols = []\n",
    "possible_pdv_cols = []\n",
    "possible_sku_cols = []\n",
    "possible_qty_cols = []\n",
    "\n",
    "for col in df_raw.columns:\n",
    "    col_lower = col.lower()\n",
    "    \n",
    "    # Colunas de data/tempo\n",
    "    if any(word in col_lower for word in ['data', 'date', 'time', 'semana', 'week', 'mes', 'month']):\n",
    "        possible_date_cols.append(col)\n",
    "    \n",
    "    # Colunas de PDV/loja\n",
    "    if any(word in col_lower for word in ['pdv', 'loja', 'store', 'ponto', 'unidade']):\n",
    "        possible_pdv_cols.append(col)\n",
    "    \n",
    "    # Colunas de produto/SKU\n",
    "    if any(word in col_lower for word in ['sku', 'produto', 'product', 'item', 'codigo']):\n",
    "        possible_sku_cols.append(col)\n",
    "    \n",
    "    # Colunas de quantidade/vendas\n",
    "    if any(word in col_lower for word in ['qtd', 'quantidade', 'qty', 'vendas', 'sales', 'volume']):\n",
    "        possible_qty_cols.append(col)\n",
    "\n",
    "print(f\"\\nüéØ Colunas identificadas:\")\n",
    "print(f\"   üìÖ Datas: {possible_date_cols}\")\n",
    "print(f\"   üè™ PDV: {possible_pdv_cols}\")\n",
    "print(f\"   üì¶ SKU/Produto: {possible_sku_cols}\")\n",
    "print(f\"   üìà Quantidade/Vendas: {possible_qty_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Estrutura√ß√£o dos Dados para An√°lise Temporal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚öôÔ∏è ESTRUTURA√á√ÉO PARA AN√ÅLISE TEMPORAL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Criar estrutura padronizada assumindo estrutura t√≠pica de dados de vendas\n",
    "df = df_raw.copy()\n",
    "\n",
    "# Mapear colunas automaticamente ou criar estrutura simulada\n",
    "if possible_date_cols:\n",
    "    date_col = possible_date_cols[0]\n",
    "    print(f\"üìÖ Usando coluna de data: {date_col}\")\n",
    "else:\n",
    "    # Simular datas de 2022 se n√£o houver coluna de data\n",
    "    print(\"üìÖ Simulando estrutura temporal para 2022...\")\n",
    "    dates_2022 = pd.date_range('2022-01-01', '2022-12-31', freq='D')\n",
    "    np.random.seed(42)\n",
    "    df['data_venda'] = np.random.choice(dates_2022, size=len(df))\n",
    "    date_col = 'data_venda'\n",
    "\n",
    "if possible_pdv_cols:\n",
    "    pdv_col = possible_pdv_cols[0]\n",
    "else:\n",
    "    # Simular PDVs se n√£o existir\n",
    "    np.random.seed(42)\n",
    "    df['pdv'] = np.random.choice([1023, 1045, 1067, 1089, 1012, 1156, 1198, 1234, 1345, 1456], size=len(df))\n",
    "    pdv_col = 'pdv'\n",
    "\n",
    "if possible_sku_cols:\n",
    "    sku_col = possible_sku_cols[0]\n",
    "else:\n",
    "    # Simular SKUs se n√£o existir\n",
    "    np.random.seed(42)\n",
    "    skus = [f\"SKU_{i:04d}\" for i in range(100, 500)]\n",
    "    df['sku'] = np.random.choice(skus, size=len(df))\n",
    "    sku_col = 'sku'\n",
    "\n",
    "if possible_qty_cols:\n",
    "    qty_col = possible_qty_cols[0]\n",
    "else:\n",
    "    # Simular quantidades baseadas em padr√µes realistas\n",
    "    np.random.seed(42)\n",
    "    # Quantidade base com varia√ß√£o sazonal e aleat√≥ria\n",
    "    base_qty = np.random.poisson(15, size=len(df))\n",
    "    seasonal_factor = np.sin(2 * np.pi * np.arange(len(df)) / 365) * 3 + 1\n",
    "    df['quantidade_vendida'] = np.maximum(1, (base_qty * seasonal_factor).astype(int))\n",
    "    qty_col = 'quantidade_vendida'\n",
    "\n",
    "print(f\"‚úì Colunas mapeadas:\")\n",
    "print(f\"   üìÖ Data: {date_col}\")\n",
    "print(f\"   üè™ PDV: {pdv_col}\")\n",
    "print(f\"   üì¶ SKU: {sku_col}\")\n",
    "print(f\"   üìà Quantidade: {qty_col}\")\n",
    "\n",
    "# Converter data se necess√°rio\n",
    "if df[date_col].dtype == 'object':\n",
    "    try:\n",
    "        df[date_col] = pd.to_datetime(df[date_col])\n",
    "        print(\"‚úì Coluna de data convertida para datetime\")\n",
    "    except:\n",
    "        print(\"‚ö†Ô∏è Erro na convers√£o de data - usando √≠ndices temporais\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Agrega√ß√£o dos Dados por Semana/PDV/SKU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä AGREGA√á√ÉO DOS DADOS POR SEMANA/PDV/SKU\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Criar coluna de semana\n",
    "df['ano'] = df[date_col].dt.year\n",
    "df['semana'] = df[date_col].dt.isocalendar().week\n",
    "df['ano_semana'] = df['ano'].astype(str) + '_S' + df['semana'].astype(str).str.zfill(2)\n",
    "\n",
    "# Agrega√ß√£o por semana, PDV e SKU\n",
    "df_agg = df.groupby(['ano', 'semana', 'ano_semana', pdv_col, sku_col]).agg({\n",
    "    qty_col: 'sum',\n",
    "    date_col: 'min'  # Para manter refer√™ncia de data\n",
    "}).reset_index()\n",
    "\n",
    "df_agg.rename(columns={\n",
    "    pdv_col: 'pdv',\n",
    "    sku_col: 'sku', \n",
    "    qty_col: 'quantidade',\n",
    "    date_col: 'data_referencia'\n",
    "}, inplace=True)\n",
    "\n",
    "print(f\"üìä Dados agregados: {df_agg.shape[0]} registros\")\n",
    "print(f\"üìÖ Per√≠odo: {df_agg['ano'].min()} a {df_agg['ano'].max()}\")\n",
    "print(f\"üè™ PDVs √∫nicos: {df_agg['pdv'].nunique()}\")\n",
    "print(f\"üì¶ SKUs √∫nicos: {df_agg['sku'].nunique()}\")\n",
    "print(f\"üìà Vendas totais: {df_agg['quantidade'].sum():,}\")\n",
    "\n",
    "# Filtrar apenas dados de 2022 (base hist√≥rica)\n",
    "df_2022 = df_agg[df_agg['ano'] == 2022].copy()\n",
    "print(f\"\\n‚úì Base hist√≥rica 2022: {df_2022.shape[0]} registros\")\n",
    "\n",
    "# Mostrar amostra dos dados\n",
    "display(df_2022.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. An√°lise Explorat√≥ria dos Dados de Vendas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìà AN√ÅLISE EXPLORAT√ìRIA DOS DADOS DE VENDAS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Estat√≠sticas gerais\n",
    "print(\"üìä Estat√≠sticas Gerais 2022:\")\n",
    "print(f\"   ‚Ä¢ Vendas totais: {df_2022['quantidade'].sum():,} unidades\")\n",
    "print(f\"   ‚Ä¢ M√©dia semanal por PDV/SKU: {df_2022['quantidade'].mean():.1f} unidades\")\n",
    "print(f\"   ‚Ä¢ Mediana: {df_2022['quantidade'].median():.1f} unidades\")\n",
    "print(f\"   ‚Ä¢ Desvio padr√£o: {df_2022['quantidade'].std():.1f}\")\n",
    "\n",
    "# Top PDVs e SKUs\n",
    "top_pdvs = df_2022.groupby('pdv')['quantidade'].sum().sort_values(ascending=False).head(10)\n",
    "top_skus = df_2022.groupby('sku')['quantidade'].sum().sort_values(ascending=False).head(10)\n",
    "\n",
    "print(f\"\\nüè™ Top 10 PDVs por volume:\")\n",
    "for pdv, qty in top_pdvs.items():\n",
    "    print(f\"   PDV {pdv}: {qty:,} unidades\")\n",
    "\n",
    "print(f\"\\nüì¶ Top 5 SKUs por volume:\")\n",
    "for sku, qty in top_skus.head(5).items():\n",
    "    print(f\"   {sku}: {qty:,} unidades\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiza√ß√µes\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('An√°lise Explorat√≥ria - Dados de Vendas 2022', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Vendas por semana\n",
    "vendas_semana = df_2022.groupby('semana')['quantidade'].sum()\n",
    "axes[0, 0].plot(vendas_semana.index, vendas_semana.values, marker='o', linewidth=2)\n",
    "axes[0, 0].set_title('Vendas Totais por Semana (2022)')\n",
    "axes[0, 0].set_xlabel('Semana')\n",
    "axes[0, 0].set_ylabel('Quantidade Total')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Distribui√ß√£o de quantidades\n",
    "axes[0, 1].hist(df_2022['quantidade'], bins=50, alpha=0.7, edgecolor='black')\n",
    "axes[0, 1].set_title('Distribui√ß√£o das Quantidades Vendidas')\n",
    "axes[0, 1].set_xlabel('Quantidade')\n",
    "axes[0, 1].set_ylabel('Frequ√™ncia')\n",
    "axes[0, 1].set_yscale('log')\n",
    "\n",
    "# 3. Top 10 PDVs\n",
    "axes[0, 2].barh(range(len(top_pdvs)), top_pdvs.values)\n",
    "axes[0, 2].set_yticks(range(len(top_pdvs)))\n",
    "axes[0, 2].set_yticklabels([f'PDV {p}' for p in top_pdvs.index])\n",
    "axes[0, 2].set_title('Top 10 PDVs - Volume Total')\n",
    "axes[0, 2].set_xlabel('Quantidade Total')\n",
    "\n",
    "# 4. Sazonalidade mensal\n",
    "df_2022['mes'] = df_2022['data_referencia'].dt.month\n",
    "vendas_mes = df_2022.groupby('mes')['quantidade'].sum()\n",
    "meses_nomes = [calendar.month_name[i] for i in vendas_mes.index]\n",
    "axes[1, 0].bar(range(len(vendas_mes)), vendas_mes.values)\n",
    "axes[1, 0].set_title('Vendas por M√™s (2022)')\n",
    "axes[1, 0].set_xlabel('M√™s')\n",
    "axes[1, 0].set_ylabel('Quantidade Total')\n",
    "axes[1, 0].set_xticks(range(len(vendas_mes)))\n",
    "axes[1, 0].set_xticklabels([m[:3] for m in meses_nomes], rotation=45)\n",
    "\n",
    "# 5. Boxplot por PDV (top 5)\n",
    "top_5_pdvs = top_pdvs.index[:5]\n",
    "data_boxplot = [df_2022[df_2022['pdv'] == pdv]['quantidade'].values for pdv in top_5_pdvs]\n",
    "axes[1, 1].boxplot(data_boxplot, labels=[f'PDV\\n{p}' for p in top_5_pdvs])\n",
    "axes[1, 1].set_title('Distribui√ß√£o de Vendas - Top 5 PDVs')\n",
    "axes[1, 1].set_ylabel('Quantidade')\n",
    "\n",
    "# 6. Tend√™ncia temporal geral\n",
    "vendas_acum = df_2022.groupby('ano_semana')['quantidade'].sum().cumsum()\n",
    "axes[1, 2].plot(range(len(vendas_acum)), vendas_acum.values, color='green', linewidth=2)\n",
    "axes[1, 2].set_title('Vendas Acumuladas 2022')\n",
    "axes[1, 2].set_xlabel('Semanas')\n",
    "axes[1, 2].set_ylabel('Quantidade Acumulada')\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Engineering para Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚öôÔ∏è FEATURE ENGINEERING PARA FORECASTING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def criar_features_temporais(df_base):\n",
    "    \"\"\"Criar features temporais para modelos de forecast\"\"\"\n",
    "    df_features = df_base.copy()\n",
    "    \n",
    "    # Features temporais b√°sicas\n",
    "    df_features['semana_ano'] = df_features['semana']\n",
    "    df_features['trimestre'] = ((df_features['semana'] - 1) // 13) + 1\n",
    "    \n",
    "    # Features sazonais\n",
    "    df_features['seno_semana'] = np.sin(2 * np.pi * df_features['semana'] / 52)\n",
    "    df_features['cosseno_semana'] = np.cos(2 * np.pi * df_features['semana'] / 52)\n",
    "    \n",
    "    # Features de tend√™ncia\n",
    "    df_features['tendencia'] = range(len(df_features))\n",
    "    \n",
    "    # Inicializar colunas de lag\n",
    "    df_features['lag_1'] = np.nan\n",
    "    df_features['lag_2'] = np.nan\n",
    "    df_features['lag_4'] = np.nan\n",
    "    df_features['media_4'] = np.nan\n",
    "    df_features['media_8'] = np.nan\n",
    "    \n",
    "    # Features de lag (vendas anteriores) por PDV/SKU\n",
    "    # Fazendo por grupo para evitar problemas de alinhamento\n",
    "    grouped = df_features.groupby(['pdv', 'sku'])\n",
    "    df_features[['lag_1','lag_2','lag_4','media_4','media_8']] = grouped['quantidade'].apply(\n",
    "        lambda x: pd.DataFrame({\n",
    "            'lag_1': x.shift(1),\n",
    "            'lag_2': x.shift(2),\n",
    "            'lag_4': x.shift(4),\n",
    "            'media_4': x.rolling(4).mean(),\n",
    "            'media_8': x.rolling(8).mean()\n",
    "        })\n",
    "    ).reset_index(level=[0,1], drop=True)\n",
    "    \n",
    "    # Preencher NaNs com m√©dias gerais\n",
    "    mean_qty = df_features['quantidade'].mean()\n",
    "    df_features['lag_1'].fillna(mean_qty, inplace=True)\n",
    "    df_features['lag_2'].fillna(mean_qty, inplace=True)\n",
    "    df_features['lag_4'].fillna(mean_qty, inplace=True)\n",
    "    df_features['media_4'].fillna(mean_qty, inplace=True)\n",
    "    df_features['media_8'].fillna(mean_qty, inplace=True)\n",
    "    \n",
    "    # Features de PDV e SKU (encoding)\n",
    "    le_pdv = LabelEncoder()\n",
    "    le_sku = LabelEncoder()\n",
    "    df_features['pdv_encoded'] = le_pdv.fit_transform(df_features['pdv'])\n",
    "    df_features['sku_encoded'] = le_sku.fit_transform(df_features['sku'])\n",
    "    \n",
    "    return df_features, le_pdv, le_sku\n",
    "\n",
    "# Aplicar feature engineering\n",
    "print(\"üîß Criando features temporais...\")\n",
    "df_features, encoder_pdv, encoder_sku = criar_features_temporais(df_2022)\n",
    "\n",
    "# Colunas de features para o modelo\n",
    "feature_cols = ['semana_ano', 'trimestre', 'seno_semana', 'cosseno_semana', \n",
    "                'tendencia', 'lag_1', 'lag_2', 'lag_4', 'media_4', 'media_8',\n",
    "                'pdv_encoded', 'sku_encoded']\n",
    "\n",
    "print(f\"‚úì Features criadas: {len(feature_cols)}\")\n",
    "print(f\"‚úì Dataset com features: {df_features.shape}\")\n",
    "\n",
    "# Mostrar correla√ß√£o das features\n",
    "corr_matrix = df_features[feature_cols + ['quantidade']].corr()\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, square=True)\n",
    "plt.title('Matriz de Correla√ß√£o - Features vs Quantidade')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Divis√£o dos Dados para Treinamento (Time Series Split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä DIVIS√ÉO DOS DADOS PARA TREINAMENTO\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Ordenar por tempo\n",
    "df_features = df_features.sort_values(['ano', 'semana', 'pdv', 'sku']).reset_index(drop=True)\n",
    "\n",
    "# Usar TimeSeriesSplit para respeitar ordem temporal\n",
    "# √öltimas 8 semanas como teste (aproximadamente 15% dos dados de 2022)\n",
    "total_semanas = df_features['semana'].nunique()\n",
    "semanas_teste = 8\n",
    "semana_corte = df_features['semana'].max() - semanas_teste\n",
    "\n",
    "print(f\"üìÖ Total de semanas em 2022: {total_semanas}\")\n",
    "print(f\"üìä Semanas para treino: at√© semana {semana_corte}\")\n",
    "print(f\"üìä Semanas para teste: {semanas_teste} √∫ltimas semanas\")\n",
    "\n",
    "# Dividir dados\n",
    "train_mask = df_features['semana'] <= semana_corte\n",
    "test_mask = df_features['semana'] > semana_corte\n",
    "\n",
    "X_train = df_features[train_mask][feature_cols]\n",
    "y_train = df_features[train_mask]['quantidade']\n",
    "X_test = df_features[test_mask][feature_cols]\n",
    "y_test = df_features[test_mask]['quantidade']\n",
    "\n",
    "print(f\"‚úì Dados de treino: {X_train.shape[0]} registros\")\n",
    "print(f\"‚úì Dados de teste: {X_test.shape[0]} registros\")\n",
    "\n",
    "# Remover qualquer NaN restante\n",
    "mask_train = ~(X_train.isnull().any(axis=1) | y_train.isnull())\n",
    "mask_test = ~(X_test.isnull().any(axis=1) | y_test.isnull())\n",
    "\n",
    "X_train = X_train[mask_train]\n",
    "y_train = y_train[mask_train]\n",
    "X_test = X_test[mask_test]\n",
    "y_test = y_test[mask_test]\n",
    "\n",
    "print(f\"‚úì Ap√≥s limpeza - Treino: {X_train.shape[0]}, Teste: {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Compara√ß√£o de Modelos de Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ü§ñ COMPARA√á√ÉO DE MODELOS DE FORECASTING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Modelos espec√≠ficos para regress√£o/forecasting\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Ridge Regression': Ridge(alpha=1.0),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42),\n",
    "    'SVR': SVR(kernel='rbf', C=1.0)\n",
    "}\n",
    "\n",
    "# Adicionar XGBoost se dispon√≠vel\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    models['XGBoost'] = xgb.XGBRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è XGBoost n√£o dispon√≠vel\")\n",
    "\n",
    "# Resultados\n",
    "results = {}\n",
    "trained_models = {}\n",
    "\n",
    "print(\"üîÑ Treinando modelos de forecasting...\")\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nüìà Treinando {name}...\")\n",
    "    \n",
    "    try:\n",
    "        # Treinar modelo\n",
    "        if name == 'SVR':\n",
    "            # SVR precisa de normaliza√ß√£o\n",
    "            scaler = StandardScaler()\n",
    "            X_train_scaled = scaler.fit_transform(X_train)\n",
    "            X_test_scaled = scaler.transform(X_test)\n",
    "            \n",
    "            model.fit(X_train_scaled, y_train)\n",
    "            y_pred = model.predict(X_test_scaled)\n",
    "            \n",
    "            # Armazenar scaler junto com o modelo\n",
    "            trained_models[name] = (model, scaler)\n",
    "        else:\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "            trained_models[name] = model\n",
    "        \n",
    "        # Calcular m√©tricas\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        \n",
    "        # MAPE (Mean Absolute Percentage Error)\n",
    "        mape = np.mean(np.abs((y_test - y_pred) / np.maximum(y_test, 1))) * 100\n",
    "        \n",
    "        results[name] = {\n",
    "            'MAE': mae,\n",
    "            'MSE': mse,\n",
    "            'RMSE': rmse,\n",
    "            'R¬≤': r2,\n",
    "            'MAPE': mape\n",
    "        }\n",
    "        \n",
    "        print(f\"   MAE: {mae:.2f}\")\n",
    "        print(f\"   RMSE: {rmse:.2f}\")\n",
    "        print(f\"   R¬≤: {r2:.4f}\")\n",
    "        print(f\"   MAPE: {mape:.2f}%\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Erro: {e}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualiza√ß√£o dos Resultados dos Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä VISUALIZA√á√ÉO DOS RESULTADOS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# DataFrame com resultados\n",
    "results_df = pd.DataFrame(results).T\n",
    "\n",
    "# Plotar compara√ß√£o\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('Compara√ß√£o de Modelos de Forecasting', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. MAE\n",
    "axes[0, 0].bar(results_df.index, results_df['MAE'], color='skyblue')\n",
    "axes[0, 0].set_title('Mean Absolute Error (MAE)')\n",
    "axes[0, 0].set_ylabel('MAE')\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 2. RMSE\n",
    "axes[0, 1].bar(results_df.index, results_df['RMSE'], color='lightcoral')\n",
    "axes[0, 1].set_title('Root Mean Square Error (RMSE)')\n",
    "axes[0, 1].set_ylabel('RMSE')\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 3. R¬≤\n",
    "axes[1, 0].bar(results_df.index, results_df['R¬≤'], color='lightgreen')\n",
    "axes[1, 0].set_title('Coeficiente de Determina√ß√£o (R¬≤)')\n",
    "axes[1, 0].set_ylabel('R¬≤')\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 4. MAPE\n",
    "axes[1, 1].bar(results_df.index, results_df['MAPE'], color='orange')\n",
    "axes[1, 1].set_title('Mean Absolute Percentage Error (MAPE)')\n",
    "axes[1, 1].set_ylabel('MAPE (%)')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Tabela de resultados\n",
    "print(\"\\nüìä TABELA COMPARATIVA DE RESULTADOS:\")\n",
    "display(results_df.round(4))\n",
    "\n",
    "# Selecionar melhor modelo (menor RMSE)\n",
    "best_model_name = results_df['RMSE'].idxmin()\n",
    "print(f\"\\nüèÜ MELHOR MODELO: {best_model_name}\")\n",
    "print(f\"üìä RMSE: {results_df.loc[best_model_name, 'RMSE']:.2f}\")\n",
    "print(f\"üìä R¬≤: {results_df.loc[best_model_name, 'R¬≤']:.4f}\")\n",
    "print(f\"üìä MAPE: {results_df.loc[best_model_name, 'MAPE']:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. An√°lise Detalhada do Melhor Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîç AN√ÅLISE DETALHADA DO MELHOR MODELO\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Obter modelo e fazer predi√ß√µes\n",
    "best_model = trained_models[best_model_name]\n",
    "\n",
    "if best_model_name == 'SVR':\n",
    "    model, scaler = best_model\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    y_pred_best = model.predict(X_test_scaled)\n",
    "else:\n",
    "    model = best_model\n",
    "    y_pred_best = model.predict(X_test)\n",
    "\n",
    "# An√°lise de res√≠duos\n",
    "residuals = y_test - y_pred_best\n",
    "\n",
    "# Visualiza√ß√µes detalhadas\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle(f'An√°lise Detalhada - {best_model_name}', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Predito vs Real\n",
    "axes[0, 0].scatter(y_test, y_pred_best, alpha=0.6)\n",
    "axes[0, 0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "axes[0, 0].set_xlabel('Valores Reais')\n",
    "axes[0, 0].set_ylabel('Valores Preditos')\n",
    "axes[0, 0].set_title('Predito vs Real')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Distribui√ß√£o dos res√≠duos\n",
    "axes[0, 1].hist(residuals, bins=30, alpha=0.7, edgecolor='black')\n",
    "axes[0, 1].set_xlabel('Res√≠duos')\n",
    "axes[0, 1].set_ylabel('Frequ√™ncia')\n",
    "axes[0, 1].set_title('Distribui√ß√£o dos Res√≠duos')\n",
    "axes[0, 1].axvline(x=0, color='red', linestyle='--')\n",
    "\n",
    "# 3. Res√≠duos vs Preditos\n",
    "axes[1, 0].scatter(y_pred_best, residuals, alpha=0.6)\n",
    "axes[1, 0].set_xlabel('Valores Preditos')\n",
    "axes[1, 0].set_ylabel('Res√≠duos')\n",
    "axes[1, 0].set_title('Res√≠duos vs Preditos')\n",
    "axes[1, 0].axhline(y=0, color='red', linestyle='--')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Feature Importance (se dispon√≠vel)\n",
    "if hasattr(model, 'feature_importances_'):\n",
    "    importances = model.feature_importances_\n",
    "    feature_names = feature_cols\n",
    "    \n",
    "    # Ordenar features por import√¢ncia\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    \n",
    "    axes[1, 1].bar(range(len(importances)), importances[indices])\n",
    "    axes[1, 1].set_title('Feature Importances')\n",
    "    axes[1, 1].set_xlabel('Features')\n",
    "    axes[1, 1].set_ylabel('Import√¢ncia')\n",
    "    axes[1, 1].set_xticks(range(len(importances)))\n",
    "    axes[1, 1].set_xticklabels([feature_names[i] for i in indices], rotation=45)\n",
    "elif hasattr(model, 'coef_'):\n",
    "    coef = np.abs(model.coef_)\n",
    "    feature_names = feature_cols\n",
    "    \n",
    "    axes[1, 1].bar(range(len(coef)), coef)\n",
    "    axes[1, 1].set_title('Feature Coefficients (Absolute)')\n",
    "    axes[1, 1].set_xlabel('Features')\n",
    "    axes[1, 1].set_ylabel('Coeficiente Absoluto')\n",
    "    axes[1, 1].set_xticks(range(len(coef)))\n",
    "    axes[1, 1].set_xticklabels(feature_names, rotation=45)\n",
    "else:\n",
    "    axes[1, 1].text(0.5, 0.5, 'Feature importance n√£o dispon√≠vel\\npara este modelo', \n",
    "                   ha='center', va='center', transform=axes[1, 1].transAxes)\n",
    "    axes[1, 1].set_title('Feature Importance')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Gera√ß√£o de Previs√µes para Janeiro 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéØ GERA√á√ÉO DE PREVIS√ïES PARA JANEIRO 2023\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def gerar_previsoes_janeiro_2023(modelo_treinado, df_historico, pdvs_ativos=None, skus_ativos=None, modelo_name=None):\n",
    "    \"\"\"\n",
    "    Gerar previs√µes para as 5 primeiras semanas de janeiro 2023\n",
    "    \"\"\"\n",
    "    print(\"üîÑ Gerando previs√µes para janeiro 2023...\")\n",
    "    \n",
    "    # Definir semanas de janeiro 2023 (semanas 1-5 de 2023)\n",
    "    semanas_2023 = [1, 2, 3, 4, 5]\n",
    "    \n",
    "    # Obter PDVs e SKUs √∫nicos do hist√≥rico se n√£o especificado\n",
    "    if pdvs_ativos is None:\n",
    "        pdvs_ativos = df_historico['pdv'].unique()\n",
    "    \n",
    "    if skus_ativos is None:\n",
    "        # Usar apenas os SKUs mais vendidos para reduzir complexidade\n",
    "        top_skus = df_historico.groupby('sku')['quantidade'].sum().sort_values(ascending=False)\n",
    "        skus_ativos = top_skus.head(50).index.tolist()  # Top 50 SKUs\n",
    "    \n",
    "    print(f\"üìä Gerando previs√µes para:\")\n",
    "    print(f\"   ‚Ä¢ {len(semanas_2023)} semanas\")\n",
    "    print(f\"   ‚Ä¢ {len(pdvs_ativos)} PDVs\")\n",
    "    print(f\"   ‚Ä¢ {len(skus_ativos)} SKUs\")\n",
    "    \n",
    "    previsoes = []\n",
    "    \n",
    "    for semana in semanas_2023:\n",
    "        for pdv in pdvs_ativos:\n",
    "            for sku in skus_ativos:\n",
    "                try:\n",
    "                    # Obter dados hist√≥ricos para este PDV/SKU\n",
    "                    hist_pdv_sku = df_historico[\n",
    "                        (df_historico['pdv'] == pdv) & \n",
    "                        (df_historico['sku'] == sku)\n",
    "                    ].sort_values('semana')\n",
    "                    \n",
    "                    if len(hist_pdv_sku) == 0:\n",
    "                        # Se n√£o h√° hist√≥rico, usar m√©dias gerais\n",
    "                        lag_1 = df_historico['quantidade'].mean()\n",
    "                        lag_2 = df_historico['quantidade'].mean()\n",
    "                        lag_4 = df_historico['quantidade'].mean()\n",
    "                        media_4 = df_historico['quantidade'].mean()\n",
    "                        media_8 = df_historico['quantidade'].mean()\n",
    "                    else:\n",
    "                        # Usar √∫ltimos valores do hist√≥rico\n",
    "                        lag_1 = hist_pdv_sku['quantidade'].iloc[-1] if len(hist_pdv_sku) >= 1 else df_historico['quantidade'].mean()\n",
    "                        lag_2 = hist_pdv_sku['quantidade'].iloc[-2] if len(hist_pdv_sku) >= 2 else lag_1\n",
    "                        lag_4 = hist_pdv_sku['quantidade'].iloc[-4] if len(hist_pdv_sku) >= 4 else lag_1\n",
    "                        media_4 = hist_pdv_sku['quantidade'].tail(4).mean() if len(hist_pdv_sku) >= 4 else lag_1\n",
    "                        media_8 = hist_pdv_sku['quantidade'].tail(8).mean() if len(hist_pdv_sku) >= 8 else media_4\n",
    "                    \n",
    "                    # Criar features para a predi√ß√£o\n",
    "                    features = {\n",
    "                        'semana_ano': semana,\n",
    "                        'trimestre': 1,  # Janeiro est√° no Q1\n",
    "                        'seno_semana': np.sin(2 * np.pi * semana / 52),\n",
    "                        'cosseno_semana': np.cos(2 * np.pi * semana / 52),\n",
    "                        'tendencia': 53 + semana - 1,  # Continua√ß√£o da tend√™ncia de 2022 (exemplo)\n",
    "                        'lag_1': lag_1,\n",
    "                        'lag_2': lag_2,\n",
    "                        'lag_4': lag_4,\n",
    "                        'media_4': media_4,\n",
    "                        'media_8': media_8,\n",
    "                        'pdv_encoded': int(encoder_pdv.transform([pdv])[0]) if pdv in encoder_pdv.classes_ else 0,\n",
    "                        'sku_encoded': int(encoder_sku.transform([sku])[0]) if sku in encoder_sku.classes_ else 0\n",
    "                    }\n",
    "                    \n",
    "                    # Converter para array na ordem correta das features\n",
    "                    X_pred = np.array([features[col] for col in feature_cols]).reshape(1, -1)\n",
    "                    \n",
    "                    # Fazer predi√ß√£o\n",
    "                    if modelo_name == 'SVR' and isinstance(modelo_treinado, tuple):\n",
    "                        modelo, scaler = modelo_treinado\n",
    "                        X_pred_scaled = scaler.transform(X_pred)\n",
    "                        pred_qty = modelo.predict(X_pred_scaled)[0]\n",
    "                    else:\n",
    "                        pred_qty = modelo_treinado.predict(X_pred)[0]\n",
    "                    \n",
    "                    # Garantir que a predi√ß√£o seja positiva e realista\n",
    "                    pred_qty = max(1, int(round(float(pred_qty))))\n",
    "                    \n",
    "                    previsoes.append({\n",
    "                        'semana': semana,\n",
    "                        'pdv': pdv,\n",
    "                        'produto': sku,  # Usar SKU como produto\n",
    "                        'quantidade': pred_qty\n",
    "                    })\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"‚ö†Ô∏è Erro ao prever para PDV {pdv}, SKU {sku}, Semana {semana}: {e}\")\n",
    "                    continue\n",
    "    \n",
    "    df_previsoes = pd.DataFrame(previsoes)\n",
    "    print(f\"‚úÖ {len(df_previsoes)} previs√µes geradas\")\n",
    "    \n",
    "    return df_previsoes\n",
    "\n",
    "# Gerar previs√µes\n",
    "df_previsoes_2023 = gerar_previsoes_janeiro_2023(\n",
    "    modelo_treinado=best_model,\n",
    "    df_historico=df_features,\n",
    "    pdvs_ativos=None,  # Usar todos os PDVs\n",
    "    skus_ativos=None,  # Usar top 50 SKUs\n",
    "    modelo_name=best_model_name\n",
    ")\n",
    "\n",
    "print(\"\\nüìã PREVIS√ïES PARA JANEIRO 2023:\")\n",
    "print(\"Formato: semana | pdv | produto | quantidade\")\n",
    "display(df_previsoes_2023.head(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. An√°lise e Valida√ß√£o das Previs√µes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä AN√ÅLISE DAS PREVIS√ïES GERADAS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Estat√≠sticas das previs√µes\n",
    "print(\"üìà Estat√≠sticas Gerais das Previs√µes:\")\n",
    "print(f\"   ‚Ä¢ Total de registros: {len(df_previsoes_2023):,}\")\n",
    "print(f\"   ‚Ä¢ Quantidade total prevista: {df_previsoes_2023['quantidade'].sum():,} unidades\")\n",
    "print(f\"   ‚Ä¢ M√©dia por registro: {df_previsoes_2023['quantidade'].mean():.1f} unidades\")\n",
    "print(f\"   ‚Ä¢ Mediana: {df_previsoes_2023['quantidade'].median():.1f} unidades\")\n",
    "print(f\"   ‚Ä¢ Desvio padr√£o: {df_previsoes_2023['quantidade'].std():.1f}\")\n",
    "\n",
    "# An√°lise por semana\n",
    "print(\"\\nüìÖ An√°lise por Semana:\")\n",
    "semana_stats = df_previsoes_2023.groupby('semana').agg({\n",
    "    'quantidade': ['sum', 'mean', 'count'],\n",
    "    'produto': 'nunique',\n",
    "    'pdv': 'nunique'\n",
    "}).round(2)\n",
    "display(semana_stats)\n",
    "\n",
    "# Top PDVs e Produtos\n",
    "top_pdvs_2023 = df_previsoes_2023.groupby('pdv')['quantidade'].sum().sort_values(ascending=False).head(10)\n",
    "top_produtos_2023 = df_previsoes_2023.groupby('produto')['quantidade'].sum().sort_values(ascending=False).head(10)\n",
    "\n",
    "print(\"\\nüè™ Top 10 PDVs - Previs√£o Janeiro 2023:\")\n",
    "for pdv, qty in top_pdvs_2023.items():\n",
    "    print(f\"   PDV {pdv}: {qty:,} unidades\")\n",
    "\n",
    "print(\"\\nüì¶ Top 10 Produtos - Previs√£o Janeiro 2023:\")\n",
    "for produto, qty in top_produtos_2023.items():\n",
    "    print(f\"   {produto}: {qty:,} unidades\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiza√ß√µes das previs√µes (c√©lula final completada para Jupyter Notebook)\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('An√°lise das Previs√µes - Janeiro 2023', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Previs√£o por semana\n",
    "semana_qty = df_previsoes_2023.groupby('semana')['quantidade'].sum().sort_index()\n",
    "axes[0, 0].bar(semana_qty.index, semana_qty.values, color='skyblue', alpha=0.8)\n",
    "axes[0, 0].set_title('Quantidade Prevista por Semana - Janeiro 2023')\n",
    "axes[0, 0].set_xlabel('Semana')\n",
    "axes[0, 0].set_ylabel('Quantidade Total')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Top 15 PDVs\n",
    "top_pdvs = df_previsoes_2023.groupby('pdv')['quantidade'].sum().nlargest(15)\n",
    "axes[0, 1].barh(range(len(top_pdvs)), top_pdvs.values, color='lightgreen', alpha=0.8)\n",
    "axes[0, 1].set_yticks(range(len(top_pdvs)))\n",
    "axes[0, 1].set_yticklabels([f'PDV {int(pdv)}' for pdv in top_pdvs.index])\n",
    "axes[0, 1].invert_yaxis()\n",
    "axes[0, 1].set_title('Top 15 PDVs - Quantidade Total Prevista')\n",
    "axes[0, 1].set_xlabel('Quantidade Total')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Top 15 Produtos\n",
    "top_produtos = df_previsoes_2023.groupby('produto')['quantidade'].sum().nlargest(15)\n",
    "axes[1, 0].barh(range(len(top_produtos)), top_produtos.values, color='salmon', alpha=0.8)\n",
    "axes[1, 0].set_yticks(range(len(top_produtos)))\n",
    "axes[1, 0].set_yticklabels([ (str(prod)[:15] + '...') if len(str(prod))>15 else str(prod) for prod in top_produtos.index ])\n",
    "axes[1, 0].invert_yaxis()\n",
    "axes[1, 0].set_title('Top 15 Produtos - Quantidade Total Prevista')\n",
    "axes[1, 0].set_xlabel('Quantidade Total')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Distribui√ß√£o das quantidades previstas (boxplot)\n",
    "sns.boxplot(y=df_previsoes_2023['quantidade'], ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Distribui√ß√£o das Quantidades Previstas')\n",
    "axes[1, 1].set_ylabel('Quantidade')\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()\n",
    "\n",
    "# 5. Exportar previs√µes para CSV\n",
    "output_file = 'previsoes_janeiro_2023.csv'\n",
    "df_previsoes_2023.to_csv(output_file, index=False)\n",
    "print(f\"\\n‚úÖ Previs√µes exportadas para {output_file}\")\n",
    "\n",
    "# 6. An√°lise final\n",
    "print(\"\\nüìä AN√ÅLISE FINAL DAS PREVIS√ïES\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total de registros: {len(df_previsoes_2023):,}\")\n",
    "print(f\"Quantidade total prevista: {df_previsoes_2023['quantidade'].sum():,} unidades\")\n",
    "print(f\"M√©dia por registro: {df_previsoes_2023['quantidade'].mean():.1f} unidades\")\n",
    "print(f\"Mediana por registro: {df_previsoes_2023['quantidade'].median():.1f} unidades\")\n",
    "print(f\"Desvio padr√£o: {df_previsoes_2023['quantidade'].std():.1f}\")\n",
    "print(f\"PDVs √∫nicos: {df_previsoes_2023['pdv'].nunique()}\")\n",
    "print(f\"Produtos √∫nicos: {df_previsoes_2023['produto'].nunique()}\")\n",
    "print(f\"Arquivo de sa√≠da: {output_file}\")\n",
    "\n",
    "print(\"\\nüéØ PREVIS√ïES FINALIZADAS COM SUCESSO! üéØ\")\n",
    "print(\"=\"*60)\n",
    "print(\"Pr√≥ximos passos sugeridos:\")\n",
    "print(\"1. Validar as previs√µes com a equipe de neg√≥cios\")\n",
    "print(\"2. Ajustar o modelo conforme feedback\")\n",
    "print(\"3. Automatizar o pipeline de previs√£o para execu√ß√£o peri√≥dica\")\n",
    "print(\"4. Implementar monitoramento de desempenho do modelo em produ√ß√£o\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
